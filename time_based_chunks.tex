\chapter{Optimisation through spatial and temporal division}\label{ch:Chunks}

As Whitted already noted in 1980~\cite{Wh80}, even in a ray tracing system with static checks,
intersection calculations take up the vast majority of processing time (between 75-95\% in Whitted's case).
Since this effect will only increase with more expensive intersection checks as discussed in \autoref{sec:IntersectionCost},
the amount of checks run per ray should be reduced as much as possible.
A method for this will be evaluated in this chapter.

\section{Chunks vs. Bounding Volumes}

One common optimisation for ray tracing systems is to limit the amount of intersection calculations by eliminating
objects the ray cannot intersect with in a simpler way before running proper checks.
There are two general sets of methods used for this:
\newline
Bounding Volume Hierarchies (BVHs), first proposed by Clarke~\cite{Cl76},
work by enclosing each object in the scene within a volume containing it.
This bounding volume uses a simpler geometric primitive that allows for faster intersection checks than the object itself,
usually quadric surfaces or spheres.
These bounding volumes are then grouped into bigger bounding volumes, forming a hierarchical tree structure.
Rays then walk down the tree structure, checking for intersections with the corresponding bounding volumes.
If a ray does not intersect with a branch's bounding volume,
any objects within that branch can be ignored for further intersection checks.
\newline
Another method first proposed by Fujimoto and Iwata~\cite{FI85}
instead divides a scene into separate cells (chunks),
with each chunk keeping a list of which objects are inside it.
Rays can then traverse from chunk to chunk along their trajectory
and only check for intersections with the objects contained in the chunk they are currently in.
\newline
Since objects can move around the scene, using one of these methods without changes becomes inefficient.
If, for example, a receiver moves from one end of the scene to the other over the course of ten seconds,
its bounding volume would extend over all of that distance for the entirety for the scene, despite it not touching
the majority of it for the most part.
Similarly, it would be kept in its starting position's chunk for the entirety of the scene despite leaving that area early,
making for needless intersection checks if a ray enters that area at a later time.
\newline
For this use case, chunks become a lot more efficient than BVHs:
When taking movement over time into account, each object would need separate bounding volumes for separate segments of time,
forcing a ray to not just check one bounding volume, but multiple per object.
This also means that in order to be able to create meaningful bounding volume hierarchies,
each object's bounding volumes would need to be separated at the same points in time,
which can lead to redundancies if objects move at different times.
Calculating a useful BVH becomes almost impossible.
\newline
The amount of chunks, in turn, does not change:
They can be adapted simply by storing not just which objects are inside them,
but also when each object enters and exits the chunk.
If chunk contents are calculated correctly,
this means that no intersection checks take place for objects that aren't inside the given chunk at the given time.

\section{Data Structure}

In a simple system, a chunk stores a list where each entry represents an object inside it.
To accommodate for objects moving in and out of chunks, entries will instead contain three fields:
One holding the index of the object in question,
one holding the time at which the object enters the chunk
and one holding the time at which the object leaves the chunk.
Since the latter two fields might both be optional if the scene starts/ends with the object inside the scene,
this can be nicely represented using a sum type such as Rust's Enumerators or C's union types with different states:

\begin{verbatim}
// Object stays within chunk for the whole scene
// only store the index
Static(object)
// Object enters and exits chunk at the given times
Dynamic(object, time_entry, time_exit)
// Object enters chunk at the given time
// and stays until the end
Final(object, time_entry)
\end{verbatim}

As the scene's start time is known and the state of objects before it is irrelevant,
a state containing only an exit time is not necessary as it can be modelled using the
\verb|Dynamic| state with a \verb|time_entry| matching the scene's starting time.
Using a more common product type system, chunk entries can instead be represented as a
struct or class where the entry and exit times are optional or nullable fields.
\newline
A ray traversing this scene can now simply check when it enters and exits a chunk
and pick out the objects to check for intersections with accordingly.
When using sum types, the space requirements for static objects only increase by one byte denoting the type's variant.
For moving objects, only up to two additional fields plus the variant field are required,
with the timestamp fields' size depending on the implementation.
Compared to the performance gains from avoiding needless intersection checks,
this additional space requirement is minimal.

\section{Calculating Chunks}

Before shooting rays through a scene,
all objects must be stored correctly within their respective chunks.
The input to a chunk calculation algorithm would thus be a set of \(n\) surfaces \(S_{0..(n-1)}\),
each with a varying number \(m\) of keyframes \(K_{0..(m-1)}\).
The information held by keyframes is the same as defined in \autoref{sec:IntersectSurface}.
\newline
Additionally, the starting coordinates of the very first chunk \verb|xmin, ymin, zmin|
as well as the chunk sizes \verb|wx, wy, wz| need to be known.
The starting coordinates are generally the lowest coordinates reached by an object in the scene.
The chunk sizes can be determined by determining a number of chunks to be used for calculation,
then calculating the difference between the scene's lowest and highest coordinates in each dimension
and dividing that by the number of chunks.
To avoid errors with objects being at the very edge of the last chunk,
it is advisable to slightly pad out the scene's lowest and highest coordinates
from the actual extremes found in objects.
\newline
A naive algorithm to place a surface in its appropriate chunks could then look like this:

\begin{verbatim}
for m in 1..(surface.num_keyframes - 1) {
    keyframe_first = surface.keyframes[m-1];
    keyframe_second = surface.keyframes[m];
    // find the highest and lowest x, y and z values
    (max_coords, min_coords) = find_max_and_min_coords(
        keyframe_first.points,
        keyframe_second.points
    );
    // floor() to round down to the index of the chunk
    chunk_min_x = floor((min_coords.x - xmin) / wx);
    chunk_min_y = floor((min_coords.y - ymin) / wy);
    chunk_min_z = floor((min_coords.z - zmin) / wz);
    chunk_max_x = floor((max_coords.x - xmin) / wx);
    chunk_max_y = floor((max_coords.y - ymin) / wy);
    chunk_max_z = floor((max_coords.z - zmin) / wz);
    for x in chunk_min_x..chunk_max_x {
        for y in chunk_min_y..chunk_max_y {
            for z in chunk_min_z..chunk_max_z {
                chunks[x][y][z].add_surface_entry(
                    {
                        time_start: keyframe_first.time,
                        time_end: keyframe_second.time,
                        index: surface.index
                    }
                );
            }
        }
    }
}
\end{verbatim}

Note that to avoid errors after the last keyframe's time,
the last keyframe then needs to also be processed on its own,
with each chunk touched by this last keyframe's version of the surface getting an according \verb|Final| entry.
This is omitted from the pseudocode of all algorithms in this section for brevity.
\newline
This naive approach comes with the problem that if an object traverses a long distance between two keyframes,
it will again be needlessly included in all chunks traversed for the entire time between those two keyframes.
An easy solution for this is to insert `pseudo-keyframes' interpolated from the actual keyframes
and running the above calculation between those.
\newline
The naive way to determine the position of these pseudo-keyframes while avoiding wrong chunk entries
would be to move through time from the first keyframe's time to the second keyframe's time,
then write new chunk entries whenever the chunks at the given time change:

\begin{verbatim}
function add_to_chunks(key_first, key_second) {
    last_time = key_first.time;
    time = key_first.time + 1;
    key_middle = interpolate(key_first, key_second, time);
    while time != key_second.time {
        if key_middle.chunks() != key_first.chunks() {
            write_chunks(
                key_first.time,
                time - 1,
                index,
                key_first.chunks()
            );
            key_first = key_middle;
            last_time = time;
        }
        time += 1;
        key_middle = interpolate(key_first, key_second, time);
    }
    write_chunks(last_time, time, index, key_second.chunks);
}
\end{verbatim}

This is obviously inefficient performance wise as keyframes need to be interpolated for every single step
despite the chunks potentially staying the same for many of said steps.
A more efficient approach could use a divide-and-conquer approach,
continually halving the range between the middling and first keyframe until they are the same,
somewhat akin to binary search:

\begin{verbatim}
function add_to_chunks(key_first, key_second) {
    time = key_first.time
    while time != key_second.time {
        time = avg(key_first.time, key_second.time);
        key_middle = interpolate(key_first, key_second, time);
        while key_middle.chunks() != key_first.chunks() {
            time = avg(key_first.time, time);
            key_middle = interpolate(
                key_first,
                key_second,
                time
            );
        }
        while key_middle.chunks() == key_first.chunks
            && time != key_second.time {
            time += 1;
            key_middle = interpolate(
                key_first,
                key_second,
                time
            );
        }
        write_chunks(
            key_first.time, 
            time - 1, 
            index,
            key_first.chunks()
        );
        key_first = key_middle;
    }
}
\end{verbatim}

This can be optimised further by fully behaving like binary search,
always halving the range between the first and middle keyframe in both directions,
but in the tests performed for this thesis,
this version of the algorithm was already fast enough for its time cost to be neglegible
compared to the intersection check/bouncing time.

\section{Traversing Chunks}\label{sec:TraversingChunks}

When using chunks, shooting rays no longer simply entails checking for intersections with all objects and choosing the earliest one.
Instead, the traversal of the ray through the separate chunks is modelled,
then intersection checks are performed for the objects in each chunk the ray enters.
\newline
Many algorithms have been developed to model rays moving from one chunk to the next~\cite{CW88}~\cite{FI85}~\cite{HT92}.
Most of them keep track of the chunk the ray is in as well as the distance the ray has already travelled in one form or another.
Adapting them to time-based chunks thus requires also keeping track of the time that has elapsed additionally to,
or instead of, the travel distance.
Additionally, the intersections within a chunk can no longer be calculated when moving to it,
as the exit time is not known at this point.
Instead, intersection checks need to be performed when moving on to the next chunk.
\newline
For this thesis and the proof-of-concept implementation,
a slightly altered version of the algorithm by Cleary and Wyvill~\cite{CW88} (called CW88 in this thesis) will be adapted.
\newline
CW88 works by keeping track of the distance a ray needs to travel until it arrives at the next chunk in each dimension
using three variables \verb|dx, dy, dz|
as well as keeping track of the distance a ray needs to travel between two chunk borders in the same dimension
in three additional variables \verb|deltax, deltay, deltaz|.
The \verb|delta*| variables are initialised based on the ray's direction cosines,
with \verb|d*| being initialised as the percentage of the corresponding \verb|delta*| variable relevant to the starting chunk.
The bounds of the scene in each dimension are stored in variables \verb|sx, sy, sz|.
Once \verb|dx >= sx| or the equivalent in another dimension becomes true,
the ray has gone out of bounds without hitting any object.
\newline
Chunks are stored in a one-dimensional array,
where a chunk at a given x-, y- and z- index is found at the index \(x \cdot n^2 + y \cdot n + z\),
with \(n\) being the number of chunks in each dimension.
This array, however, only holds a single value per chunk indicating whether it contains objects to begin with.
The chunks that do contain objects are stored in a hash map or similar data structure,
using the array index as a key.
\newline
This index isn't newly calculated with every step.
Instead, it is stored in a variable \verb|p|,
with three variables \verb|px, py, pz| storing the value to add to the index when entering a new chunk in the given dimension.
\newline
All in all, the original CW88 algorithm works as follows,
with initialisation of y- and z-related variables being omitted for brevity:

\begin{verbatim}
// init cx as direction cosine for x
// init chunkx, chunky, chunkz as chunk index
p = chunkx * pow(n, 2) + chunky * n + chunkz;
mx = pow(n,2);
chunk_start = xmin + chunkx * wx;
if cx == 0 {
    // no movement in x direction
    dx = infinity;
    sx = 0;
} else {
    if cx > 0 {
        deltax = wx/cx;
        px = mx;
        dx = (chunk_start + wx - x) / wx * deltax;
        sx = (n-chunkx) * deltax;
    } else {
        deltax = -wx/cx;
        px = -mx;
        dx = (x - chunk_start) / wx * deltax;
        sx = (chunkx + 1) * deltax;
    }
}
// init y and z variables by the same schema here
while true {
    if dx <= dy && dx <= dz {
        if dx >= sx {
            return null;
        }
        p += px;
        dx += deltax;
    } else if dy <= dx && dy <= dz {
        if dy >= sy {
            return null;
        }
        p += py;
        dy += deltay;
    } else {
        if dz >= sz {
            return null;
        }
        p += pz;
        dz += deltaz;
    }
    if !chunk_contains_objects(p) {
        continue;
    }
    intersection = intersection_check(p);
    if intersection != null {
        return intersection;
    }
}
\end{verbatim}

In order to use CW88 with time-based chunks,
new variables are introduced to also keep track of the elapsed time.
\verb|tx, ty, tz| become the equivalent of \verb|dx, dy, dz| for time
while \verb|deltatx, deltaty, deltatz| are the timed version of \verb|deltax, deltay, deltaz|.
\newline
\verb|deltatx|, \verb|deltaty|, \verb|deltatz| are simply initialised as \verb|deltax/velocity|, \verb|deltay/velocity|, \verb|deltaz/velocity|,
with \verb|velocity| representing the ray's velocity, usually the speed of sound.
\verb|tx| is initialised as \verb|dx/velocity + t0|, with \verb|t0| being the ray's starting time.
\verb|ty| and \verb|tz| can be initialised by the same scheme.
\newline
As distance travelled and time passed directly scale,
the \verb|deltat*| and \verb|t*| variables could be omitted with \verb|t*| being calculated from \verb|dx| whenever needed.
In practice, keeping them as a separate variable means that updating \verb|t*| only takes a single addition per step,
whereas the cost of calculating \verb|t*| from \verb|d*| every step would take both an addition and a division.
\newline
This does however allow for removal of both \verb|delta*| and \verb|d*|
as \verb|deltat*| and \verb|t*| can be used to track the distance to the next chunk in the same way.
The \verb|s*| variables then need to be calculated using \verb|deltat*| rather than \verb|delta*|.
\newline
Another necessary change is that, as discussed above,
the intersection check done at the end of the loop is not done for the newly entered chunk that matches \verb|p|,
but the chunk the ray just left instead.
In order to then keep the time the ray entered the last chunk,
a new variable \verb|tlast| is introduced and initialised as the ray's starting time.
\newline
The full traversal algorithm, omitting all variable initialisation for brevity, then looks as follows:

\begin{verbatim}
while true {
    if tx <= ty && tx <= tz {
        intersection = traverse_chunk(p, tx, tlast);
        if intersection != null {
            return intersection;
        }
        tlast = tx;
        p += px;
        tx += deltatx;
        if tx >= sx {
            return null;
        }
    } else if ty <= tx && ty <= tz {
        intersection = traverse_chunk(p, ty, tlast);
        if intersection != null {
            return intersection;
        }
        tlast = ty;
        p += py;
        ty += deltaty;
        if ty >= sy {
            return null;
        }
    } else {
        intersection = traverse_chunk(p, ty, tlast);
        if intersection != null {
            return intersection;
        }
        tlast = tz;
        p += pz;
        tz += deltatz;
        if tz >= sz {
            return null;
        }
	}
    if !chunk_contains_objects(p) {
        continue;
    }
    intersection = intersection_check(p);
    if intersection != null {
        return intersection;
    }
}

function traverse_chunk(p, t_dimension, tlast) {
    if !chunk_contains_objects(p) {
        continue;
    }
    intersection = intersection_check(p);
    if intersection != null 
        && intersection.time <= t_dimension
        && intersection.time >= tlast {
        return intersection;
    }
    return null;
}
\end{verbatim}

\begin{figure}[b!]
    \includesvg[width=\linewidth]{images/hit_after_chunk.svg}
    \caption[Demonstration of why only intersections inside a chunk are counted]{An example for why intersections happening outside of the ray's current chunk need to be ignored.}\label{fig:HitAfterChunk}
\end{figure}
Intersections that are outside of the entry/exit time (\verb|tlast| and \verb|t_dimension| respectively) for the current chunk are discarded.
This is to avoid errors when a ray would intersect with the given object in a chunk it will enter later,
but would also hit another object before it.
Take \autoref{fig:HitAfterChunk} as an example:
Object 1 is in all three shown chunks, whereas object 2 is only in the middle chunk.
When the ray tests for intersections with objects in the first chunk,
it finds that it does eventually hit object 1.
If it takes this as a final result and proceeds to bounce off of its intersection with object 1,
it never enters the second chunk and thus never checks for an intersection with object 2.
This leads to a wrong result as the ray should bounce off of object 2 instead.
This error can also occur with the original CW88 algorithm,
but incidentally becomes easier to fix through this adaptation.
\newline
Simply discarding intersections outside the time the ray is within a chunk comes with the downside of duplicate calculations:
If an object is inside three chunks the ray travels through, with the intersection happening in the last of those chunks,
the intersection calculation is run thrice, but tossed away two of those three times.
\newline
A solution to this could involve storing the intersections for each already checked object in a map-like structure
and only newly calculating intersections for objects that haven't been checked yet.
