\chapter{Optimisation through spatial and temporal division}\label{ch:Chunks}

As Whitted already noted in 1980~\cite{Wh80}, even in a ray tracing system with static checks,
intersection calculations take up the vast majority of processing time (between 75-95\% in Whitted's case).
Since this effect will only increase with more expensive intersection checks as discussed in \autoref{sec:IntersectionCost},
the amount of checks run per ray should be reduced as much as possible.
A method for this will be evaluated in this chapter.

\section{Chunks vs. Bounding Volumes}

One common optimisation for ray tracing systems is to limit the amount of intersection calculations by eliminating
objects the ray cannot intersect with in a simpler way before running proper checks.
There are two general sets of methods used for this:
\newline
Bounding Volume Hierarchies (BVHs), first proposed by Clarke~\cite{Cl76},
work by enclosing each object in the scene within a volume containing it.
This bounding volume uses a simpler geometric primitive that allows for faster intersection checks than the object itself,
usually quadric surfaces or spheres.
These bounding volumes are then grouped into bigger bounding volumes, forming a hierarchical tree structure.
Rays then walk down the tree structure, checking for intersections with the corresponding bounding volumes.
If a ray does not intersect with a branch's bounding volume,
any objects within that branch can be ignored for further intersection checks.
\newline
Another method first proposed as a Three Dimensional Digital Differential Analyzer by Fujimoto and Iwata~\cite{FI85},
instead divides a scene into separate cells (chunks),
with each chunk keeping a list of which objects are inside it.
Rays can then traverse from chunk to chunk along their trajectory
and only check for intersections with the objects contained in the chunk they're currently in.
\newline
Since objects can move around the scene, using one of these methods without changes becomes inefficient.
If, for example, a receiver moves from one end of the scene to the other over the course of ten seconds,
its bounding volume would extend over all of that distance for the entirety for the scene, despite it not touching
the majority of it for the most part.
Similarly, it would be kept in its starting position's chunk for the entirety of the scene despite leaving that area very early,
making for needless intersection checks if a ray enters that area at a later time.
\newline
For this use case, chunks become a lot more efficient than BVHs:
When taking movement over time into account, each object would need separate bounding volumes for separate segments of time,
forcing a ray to not just check one bounding volume, but multiple per object.
This also means that in order to be able to create meaningful bounding volume hierarchies,
each object's bounding volumes would need to be separated at the same points in time,
which can lead to redundancies if objects move at different times.
Calculating a useful BVH becomes near impossible.
\newline
The amount of chunks, in turn, does not change:
They can be adapted simply by storing not just which objects are inside them,
but also when each object enters and exits the chunk.
If chunk contents are calculated correctly,
this means that no intersection checks take place for objects that aren't inside the given chunk at the given time.

\section{Data Structure}

In a simple system, a chunk stores a list where each entry represents an object inside it.
To accommodate for objects moving in and out of chunks, entries will instead contain three fields:
One holding the index of the object in question,
one holding the time at which the object enters the chunk
and one holding the time at which the object leaves the chunk.
Since the latter two fields might both be optional if the scene starts/ends with the object inside the scene,
this can be nicely represented using a sum type such as Rust's Enumerators or C's union types with different states:

\begin{verbatim}
// Object stays within chunk for the whole scene
// only store the index
Static(object)
// Object enters and exits chunk at the given times
Dynamic(object, time_entry, time_exit)
// Object enters chunk at the given time
// and stays until the end
Final(object, time_entry)
\end{verbatim}

As the scene's start time is known and the state of objects before it is irrelevant,
a state containing only an exit time is not necessary as it can be modelled using the
\verb|Dynamic| state with a \verb|time_entry| matching the scene's starting time.
Using a more common product type system, chunk entries can instead be represented as a
struct or class where the entry and exit times are optional or nullable fields.
\newline
A ray traversing this scene can now simply check when it enters and exits a chunk
and pick out the objects to check for intersections with accordingly.
When using sum types, the space requirements for static objects only increase by one byte denoting the type's variant
(with even that potentially getting left out, as Herzog showed~\cite{He23}).
For moving objects, only up to two additional fields plus the variant field are required,
with the timestamp fields' size depending on the implementation.
Compared to the performance gains from avoiding needless intersection checks,
this additional space requirement is comparatively minimal.

\section{Calculating Chunks}

Before shooting rays through a scene,
all objects must be stored correctly within their respective chunks.
The input to a chunk calculation algorithm would thus be a set of \(n\) surfaces \(S_{0..(n-1)}\),
each with a varying number \(m\) of keyframes \(K_{0..(m-1)}\).
The information held by keyframes is the same as defined in \autoref{sec:IntersectSurface}.
\newline
Additionally, the starting coordinates of the very first chunk \verb|xmin, ymin, zmin|
as well as the chunk sizes \verb|wx, wy, wz| need to be known.
For the starting coordinates, the lowest coordinates any object in the scene ever has can be used.
The chunk sizes can be determined by determining a number of chunks to use for calculation,
then calculating the difference between the scene's lowest and highest coordinates in each dimension
and dividing that by the number of chunks.
To avoid errors with objects being at the very edge of the last chunk,
it is recommendable to slightly pad out the scene's lowest and highest coordinates
from the actual extremes found in objects.
\newline
A naive algorithm to place a surface in its appropriate chunks could then look like this:

\begin{verbatim}
for m in 1..(surface.num_keyframes - 1) {
    keyframe_first = surface.keyframes[m-1];
    keyframe_second = surface.keyframes[m];
    // find the highest and lowest x, y and z values
    (max_coords, min_coords) = find_max_and_min_coords(
        keyframe_first.points,
        keyframe_second.points
    );
    // floor() to round down to the index of the chunk
    chunk_min_x = floor((min_coords.x - xmin) / wx);
    chunk_min_y = floor((min_coords.y - ymin) / wy);
    chunk_min_z = floor((min_coords.z - zmin) / wz);
    chunk_max_x = floor((max_coords.x - xmin) / wx);
    chunk_max_y = floor((max_coords.y - ymin) / wy);
    chunk_max_z = floor((max_coords.z - zmin) / wz);
    for x in chunk_min_x..chunk_max_x {
        for y in chunk_min_y..chunk_max_y {
            for z in chunk_min_z..chunk_max_z {
                chunks[x][y][z].add_surface_entry(
                    {
                        time_start: keyframe_first.time,
                        time_end: keyframe_second.time,
                        index: surface.index
                    }
                );
            }
        }
    }
}
\end{verbatim}

Note that to avoid errors after the last keyframe's time,
the last keyframe then needs to also be processed on its own,
with each chunk touched by this last keyframe's version of the surface getting an according \verb|Final| entry.
This will be omitted from the algorithms' pseudocode for brevity.
\newline
This naive approach comes with the problem that if an object traverses a long distance between two keyframes,
it will again be needlessly included in all chunks traversed for the entirety of its lifetime.
An easy solution for this would be to insert `pseudo-keyframes' interpolated from the actual keyframes
and running the above calculation between those.
\newline
The naive way to determine the position of these pseudo-keyframes while avoiding wrong chunk entries
would be to move through time from the first keyframe's time to the second keyframe's time,
then write new chunk entries whenever the chunks at the given time change:

\begin{verbatim}
function add_to_chunks(key_first, key_second) {
	last_time = key_first.time;
	time = key_first.time + 1;
	key_middle = interpolate(key_first, key_second, time);
	while time != key_second.time {
		if key_middle.chunks() != key_first.chunks() {
			write_chunks(
                key_first.time,
                time - 1,
                index,
                key_first.chunks()
            );
			key_first = key_middle;
			last_time = time;
		}
		time += 1;
		key_middle = interpolate(key_first, key_second, time);
	}
	write_chunks(last_time, time, index, key_second.chunks);
}
\end{verbatim}

This is obviously inefficient performance wise as keyframes need to be interpolated for every single step
despite the chunks potentially staying the same for many of said steps.
A more efficient approach could use a divide-and-conquer approach,
continually halving the range between the middling and first keyframe until they are the same,
somewhat akin to binary search:

\begin{verbatim}
function add_to_chunks(key_first, key_second) {
	time = key_first.time
	while time != key_second.time {
		time = avg(key_first.time, key_second.time);
		key_middle = interpolate(key_first, key_second, time);
		
		while key_middle.chunks() != key_first.chunks() {
			time = avg(key_first.time, time);
			key_middle = interpolate(key_first, key_second, time);
		}
		while key_middle.chunks() == key_first.chunks
            && time != key_second.time {
			time += 1;
			key_middle = interpolate(key_first, key_second, time);
		}
		write_chunks(
            key_first.time, 
            time - 1, 
            index,
            key_first.chunks()
        );
		
		key_first = key_middle;
	}
}
\end{verbatim}

This can be optimised further by fully behaving like binary search,
always halving the range between the first and middle keyframe in both directions,
but in the tests performed for this thesis,
this version of the algorithm was already fast enough for its time cost to be neglegible
compared to the intersection check/bouncing time.

\section{Traversing Chunks}

When using chunks, shooting rays no longer simply entails checking for intersections with all objects and choosing the earliest one.
Instead, the traversal of the ray through the separate chunks is modelled,
then intersection checks are performed for the objects in each chunk the ray enters.
\newline
% TODO cite
Many algorithms have been developed to model rays moving from one chunk to the next.
Most of them keep track of the chunk the ray is in as well as the distance the ray has already travelled in one form or another.
Adapting them to time-based chunks thus requires also keeping track of the time that has elapsed additionally to,
or instead of, the travel distance.
Additionally, the intersections within a chunk can no longer be calculated when moving to it,
as the exit time is not known at this point.
Instead, intersection checks need to be performed when moving on to the next chunk.
\newline
For this thesis and the proof-of-concept implementation,
a slightly simplified version of the algorithm by Cleary and Wyvill~\cite{CW88} (called CW88 in this thesis) will be adapted.
\newline
CW88 works by keeping track of the distance a ray needs to travel until it arrives at the next chunk in each dimension
using three variables \verb|dx, dy, dz|
as well as keeping track of the distance a ray needs to travel between two chunk borders in the same dimension
in three additional variables \verb|deltax, deltay, deltaz|.
The bounds of the scene in each dimension are stored in variables \verb|sx, sy, sz|.
Once \verb|dn >= sn| for any dimension n, the ray has gone out of bounds without hitting any object.
\newline
Chunks are stored in a one-dimensional array,
where a chunk at a given x-, y- and z- index is found at the index \(x \cdot n^2 + y \cdot n + z\),
with \(n\) being the number of chunks in each dimension.
This array, however, only holds a single value per chunk indicating whether it contains objects to begin with.
The chunks that do contain objects are stored in a hash map or similar data structure,
using the array index as a key.
\newline
This index isn't newly calculated with every step.
Instead, it is stored in a variable \verb|p|,
with three variables \verb|px, py, pz| storing the value to add to the index when entering a new chunk in the given dimension.
\newline
All in all, the original CW88 algorithm works as follows, with variable initialisation left out for brevity:

\begin{verbatim}
while true {
	if dx <= dy && dx <= dz {
		if dx >= sx {
            return null;
        }
		p += px;
		dx += deltax;
	} else if dy <= dx && dy <= dz {
		if dy >= sy {
            return null;
        }
		p += py;
		dy += deltay;
	} else {
		if dz >= sz {
            return null;
        }
		p += pz;
		dz += deltaz;
	}
    if !chunk_contains_objects(p) {
        continue;
    }
    intersection = intersection_check(p);
    if intersection != null {
        return intersection;
    }
}
\end{verbatim}

In order to use CW88 with time-based chunks,
new variables are introduced to also keep track of the elapsed time.
\verb|tx, ty, tz| become the equivalent of \verb|dx, dy, dz| for time
while \verb|deltatx, deltaty, deltatz| are the timed version of \verb|deltax, deltay, deltaz|.